\documentclass[11pt]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{epstopdf}
%\usepackage{pdflatex}
%\usepackage{epsfig}

			% Activate to display a given date or no date
%\title{Using the Walker Circulation to connect low-level cloud changes to deep convective entrainment}
\title{Clouds and Sensitivities Across a Hierarchy of GFDL CMIP6 Models}
\author{Levi G. Silvers et al.}

\begin{document}
\maketitle

\begin{abstract}
The newest atmospheric climate model at GFDL, AM4, succeeded at significantly reducing the TOA radiative flux biases as compared to CERES observations.  Despite a relatively low top-of-atmosphere sensitivity to uniform warming of SSTs (Cess warming experiments), the corresponding coupled climate model, CM4, has high transient and equilibrium climate sensitivities.  

We will present a systematic picture of the modeled clouds across a hierarchy of model configurations which utilize this atmospheric model.  This hierarchy includes the CFMIP Aquaplanet and AMIP experiments, fully coupled model experiments (using GFDL's CM4 model) as well as additional AMIP-like experiments with particular SST patterns.  This demonstrates the large range of sensitivities that are possible from a single atmospheric climate model.   Looking at the global mean radiative feedbacks across the different model configurations as well as in the context of CMIP5 and CMIP6 models will allow us to assess to what extent the cloud feedbacks in the idealized experiments relate to the fully coupled experiments and to observed clouds.  
\end{abstract}

%\section{Key Questions}

\textbf{Key Points}
\begin{itemize}
  \item{The AM4.0 TOA cloud radiative effect shows small biases compared with CERES-EBAF observations, both versions 
  2.8 and 4.1.}
  \item{Too few clouds are simulated by AM4.0 at high, middle, and low-levels, as compared to observations from CALIPSO.}
  \item{Various configurations of a single global climate model lead to climate sensitivities that range from 2.1K (Cess) to 
  about 5K (Winton's ECS).}
\end{itemize}

\section{Introduction}

This paper has a double motivation.  First, to illustrate the wide range of sensitivities to perturbations 
that can be achieved with a single base model.  For this purpose we utilize several models from the latest suite of GFDL 
models.  This constitutes a hierarchy of configurations that ranges from an atmosphere only aquaplanet to a fully coupled 
climate model.  Previously the range of climate sensitivities and feedbacks among the large ensemble of climate models 
the world has to offer have been documented largely by examining the multi-model mean results with little attention to 
the variability of sensitivity within a single modeling framework (IPCC; Andrews et al., 2012; Ringer et al., 2014).  By 
focusing on the models from a single modeling center we hope to bring out fresh insight that is lost within a large ensemble.

A second motivation for this paper is to document the simulation of clouds in the newly developed GFDL models that 
have participated in CMIP6.  This will largely be done in the context of the amip experiment and comparisons to observations
that overlap with that time period (1979-2014).  The experiments and diagnostics used are taken from the latest 
round of CFMIP which has provided a useful framework to guide experimental design and diagnostic outputs.       

Can we shed lite on which particular component of the cloud feedbacks lead to large changes in the sensitivity?  

\section{Clouds in AM4.0: Comparison between amip and observations}

\begin{figure}
% figure generated with calip25_isccp_am4_totcld.ncl, figurenumber = 1
  \includegraphics[width=0.75\columnwidth]{cfmipfigs/clt_isccp_calipso_am4_4pan.eps}
  \caption{Comparison of total cloud fraction from AM4 with two observational data sets: ISCCP (left) and CALIPSO (right).}
  \label{fig:clt_isccp_calipso}
\end{figure}

\begin{figure}
% figure generated with calip25_isccp_am4_totcld.ncl, figurenumber = 1
  \includegraphics[width=0.95\columnwidth]{/Users/silvers/Research/cfmip_paper/testplot_oldconts.pdf}
  \caption{Comparison thick (optical depth between 3.6 and 60) thin (optical depth between 0.3 and 3.6) clouds the 
  MISR simulator in  AM4 with observations from MISR (bottom panels).}
  \label{fig:misr}
\end{figure}

\begin{figure}
% figure generated with calip25_isccp_am4_totcld.ncl, figurenumber = 3
  \includegraphics[width=0.86\columnwidth]{cfmipfigs/clt_calipso_obs_sim_am4.eps}
  \caption{Comparison of total cloud fraction.  Top shows observations from CALIPSO, processed for GCM comparison.  Middle 
  shows output from the CALIPSO simulator, bottom shows raw model output of clt without a simulator.}
  \label{fig:calipso_sim_vs_mod}
\end{figure}

Before discussing the climate sensitivities of AM4/CM4 and how they are influenced by cloud feedbacks we assess
the clouds which are simulated by AM4.0 over the recent observational period of 1979-2014.  This is done by comparing
satellite based observations with the clouds in the AMIP experiment that is part of GFDLs contribution to CMIP6.  In 
order to compare the observed and modeled clouds as consistently as possible across multiple platforms we have 
used a selection of the satellite simulators which are included in COSP as part of the efforts of the CFMIP community 
(Bodas-Salcedo et al. 2011).  This paper highlights simulators of the algorithms for three satellites based datasets: 
ISCCP (the International Satellite Cloud Climatology Project), MODIS (the Moderate Resolution Imaging Spectroradiometer), 
MISR (the Multiangle Imaging Spectro-Radiometer), and CALIPSO (the Cloud-Aerosol Lidar and Infrared Pathfinder 
Satellite Observation; Chepfer et al., 2010; Cesana and Chepfer, 2013).  By using multiple complimentary satellite 
datasets and simulators we can benefit from 
the different strengths of each satellite while largely circumventing the known problems and discrepancies of the 
observational data sets.      

Discrepancies among the many observational data sets has been extensively discussed in the literature and are 
beyond the scope of this paper.  The curious reader can find detailed discussions in Kay et al., 2012; 
Marchand et al. 2010 and?; Pincus et al., 2012; Cesana et al. ?; Stephens et al., 2018.  However, we note that ISCCP is useful due to 
the longevity of the record (1983 - 2008 in Z18 figure) and MODIS has a detection algorithm that is an improvement upon ISCCP's and
thus does a better job of detecting mid-level clouds and excluding partly filled pixels.   Of the three passive satellite products 
(ISCCP, MODIS, and MISR), MODIS is considered to have the most accurate readings of high-topped clouds.  
The height of clouds is determined with a stereo height retrieval method in MISR that is distinct from that used by
both ISCCP and MODIS.  Because of this we have more confidence in the low and mid-level cloud readings 
from MISR, as well as the cumulus clouds (Marchand et al. 2010; Zhang et al., 2019).   The CALIPSO observations
are derived from a Lidar that allows for a direct measure of vertical structure of the clouds.  

\begin{figure}
% figure generated with cloud_table_AM4fig.ncl
  \includegraphics[width=0.86\columnwidth]{cfmipfigs/ctp_vs_tau_isccp_modis_am4_obs.eps}
  \caption{Joint histograms showing the variations of cloud fraction as a function of cloud top pressure and tau.}
  \label{fig:calipso_sim_vs_mod}
\end{figure}

\begin{figure}
% figure generated with cloud_table_AM4fig.ncl
  \includegraphics[angle=90,width=0.86\columnwidth]{cfmipfigs/calip_9pan_amip_obs_vs_AM4.eps}
  \caption{Calipso scumstash.  Black line shows meridional profiles of AM4.0 cloud fraction and blue line shows
  the meridional profile from CALIPSO.}
  \label{fig:calipso_9pan}
\end{figure}


As a starting point we look at the total cloud fraction in Figure \ref{fig:clt_isccp_calipso}.  The ISCCP and 
CALIPSO products agree fairly well on the 
global mean cloud fraction (65 and 67\%, respectively).   By this measure there are too few clouds in AM4.0 
(biases of -15 and -12 \%).   The ISCCP and CALIPSO data agree well over the oceans, with most of the differences
occuring over land (particularly north of 30).    

The MISR satellite is thought to provide good estimates of low-level clouds heights and amounts.  For this reason
and for comparison with both Kay et al., 2012, and Zhang et al., 2019 we compare observations from MISR
and the AM4.0 simulator output in Figure \ref{fig:misr}.  The optical depth is used to distinguish
between the thick and thin clouds.  Consistent with comparisons with ISCCP and CALIPSO we see that AM4.0
has fewer clouds than observed.  The pattern of thick clouds observed by MISR is reproduced fairly well, except in 
the tropical Indian Ocean.  The pattern of thin clouds is not reproduced as well, particularly in the southern hemisphere 
tropics.   

The global structure of the cloud fields are shown for the upper level, mid level, and low levels in 
Figure \ref{fig:calipso_9pan}.  The longitudinally averaged cloud fraction is shown as a function of latitude in the 
left most panels.  There is a large under-estimation of cloud fraction by (AM4.0: dashed; CALIPSO: solid) between 
plus/minus 30 
degrees for both mid and low-level clouds.   AM4.0 does a much better job with the upper level tropical clouds 
although there is still a negative bias.   The global maps detail where the biases are occurring.  In the upper 
levels the clouds are underestimated roughly equally among the deep-tropical and stratocumulus regions.  In the
mid-levels the bias tends to be larger over tropical land and in the region of the inter-tropical convergence zone.  
In the mid-levels there are large biases over much of the tropical Pacific ocean, as well as in the stratocumulus
zones and the southern tropical Atlantic ocean.  

Paragraph summarizing the basic characteristics of clouds in AM4.0 as stated by Ming's papers.  We could also summarize 
what David finds about clouds in AM4.0 in his paper.

During the developmental of the AM4.0 GFDL model particular care was given to ensuring that the energy budget at 
the top of the atmosphere compared well with observations.  The data set that was most heavily leaned on to ensure 
this was the CERES-EBAF TOA radiative fluxes, version 2.8 (Loeb et al., 2009).  Based on AM4.0 data that was used for 
the CMIP6 AMIP
experiment, the total TOA flux bias is 989898, the bias in CRE 
due to longwave radiative fluxes is $-2.4 \rm{W m^{-2}} $ and the bias due to CRE of the shortwave radiative fluxes   
is $-1.1 \rm{W m^{-2}} $.  About the same time the AM4.0 development was frozen and the process of documentation 
underway, the CERES-EBAF was updated to version 4.1.  When comparing the same AM4.0 AMIP experiment to the 
newer version of CERES-EBAF data we find biases of $-2.2 \rm{W m^{-2}} $ for the longwave CRE and 
$-2.9 \rm{W m^{-2}} $ for the shortwave CRE.   Should these biases be computed using only the AMIP years that 
correspond to the CERES years?   As previously discussed (e.g. Kay et al., 2012) these longwave and shortwave CRE 
biases are difficult to interpret because model developers often tune models to match particular CERES-EBAF data 
sets despite the well known fact that different versions of the CERES-EBAF data lead to changes in the observed CRE of
several $\rm{W m^{-2}}$.   Our aim in discussing this for AM4.0 is transparency in the model development process, as 
well as to point out that both versions of the CERES-EBAF data show similar shortcomings of the spatial patters of the 
simulated clouds  from AM4.0 in the AMIP experiment.   



\section{The Role of Clouds in AM4.0/CM4.0/Aquaplanets in Determining and Influencing Climate Sensitivity}

% A paragraph, either here or earlier that clearly states to what degree and why we think that changes in clouds
% are a critical element in determining why different models have different sensitivities.  Papers that should probably
% be cited are Bony and Dufresne, 2005, Webb et al. (the convection off paper), the Bretherton review on cloud 
% feedbacks, the CFMIP paper?  It would be  nice to cite more of the original papers and less of the review papers.  
% Probably some of Zelinka's papers.  

The recent model development efforts at GFDL, and participation in CMIP6/CFMIP have provided a 
unique/valuable opportunity to analyze a group of models that are built on the same code base while 
spanning a wide range of model configurations.  The model types include (should we include any of the
efforts from the RCE or Walker Cell configurations?) an Aquaplanet, and AGCM, 
and a AOGCM.  When combined with a variety of boundary conditions this creates a hierarchy of
models which are all derived from or dependent on the same atmospheric code base (AM4.0).  It is a 
bit like a group of siblings sharing the same gene pool but at various stages of intelligence or education.      
Our hypothesis for this study is that the large range of climate sensitivities that are found within this 
group of models is in large part due to variations in the clouds (both their climatology and their feedbacks).
The clouds in these simulations are responding to a variety of different conditions including different
patterns of prescribed SST, the presence or absence of land/ice, and in the case of the aquaplanet
experiments, a lack of cloud-aerosol interactions which is present in the amip and coupled experiments.  

% Ringer et al 2014 comparison
In order to compare to the values of feedback that were reported in Ringer et al., 2014, we use the same method 
to compute the radiative feedbacks for the GFDL models.  That is, for the abrupt $4xCO_{2}$ experiment, feedbacks are 
inferred to be the slopes of a linear regression calculation using annual, global-mean top-of-atmosphere flux anomalies 
(Gregory et al., 2004).  From the atmosphere only experiments (amip-p4K, amipFuture, and aqua-p4K), the feedbacks 
are calculated as 
differences in top-of-atmosphere flux anomalies between the perturbation and control experiments.   

% Beta feedback??  relavent?
Brient and Bony (2012) proposed what they call the $\beta$ feedback as a way to quantify whether or not ' models have 
changes in low-cloud cloud radiative effects which are proportional to the strength of the cloud radiative effects of their
low clouds.'  See $\it{Stevens etal, Cookie Overview}$ for details.   That paper asks the question of whether single 
model relationships between low-cloud amounts and the sensitivity of low clouds to warming can be 
masked by changes in multi-model ensembles.  This should be checked for the GFDL model in this paper.  

The cloud radiative effect (CRE) is computed as the difference between the all-sky and clear-sky TOA fluxes.  

\begin{table}
\begin{center}
\caption{Global mean radiative feedbacks.  The values for the Abrupt 4xCO2 experiment
have been estimated from a linear fit of the years 51-300.  Compare to Ringer et al. 2014.}
    \begin{tabular}{*{6}{c}}
    \hline
    \hline
 Feedback & Aqua p4K & Amip m4K & Amip p4K & Amip Future & Abrupt 4xCO2    \\ \hline
    Net          &   -2.2  & ??  &  -1.6              & -1.8           &    -0.46 (-0.84; Tim)         \\ 
    \\
    Net CRE      & 0.3   & ??  & -0.1            & 0.1           & 0.65   \\  
    \\
    LW CRE       & 0.4   & ??  & 0.1            & 0.1           & 0.14    \\  
    \\
    SW CRE      & -0.7  & ?? & -0.1          & -0.2          & -0.5              \\  
    \\
    LW CLR       & 2.0   & ?? & 2.0             & 2.0           & 1.7             \\  
    \\
    SW CLR      & -0.1  & ?? & -0.3            & -0.3          & -0.6                   \\  \hline

    \end{tabular}\par
    %\bigskip 
    \label{tab:lambda}
\end{center}
\end{table}

% SST patterns, the pattern effect.  Should my discussion paragraph of the pattern effect be 
% the same paragraph/discussion as when I discuss Cess experiments?  
The Cess sensitivity for AM4.0 ($0.57 \, \rm{K\, W^{-1} m^2}$, Z18b, pg 26) is closer to the Cess sensitivity 
% see page 39 of Zhao et al. 2018 part 1 for discussion of Cess sensitivity.  
of AM2.1 ($0.54 \, \rm{K\, W^{-1} m^2}$) than AM3.  This is partly due to the method of converting convective cloud condensate to precipitation 
in the convection scheme.   Z18b on the same page describes a Cess sensitivity with AM4.0 when the drop number
is fixed of ($0.52 \, \rm{K\, W^{-1} m^2}$).  As discussed in Z18b and Winton et al. 2019, despite a Cess sensitivity similar to AM2.1, 
the TCR and ECS of CM4.0 are larger than those of CM2.1, and close to those of CM3.   This clearly 
shows that using the Cess sensitivity to infer the TCR or ECS is not reliable (does not work).   Is the ratio of TCR to
Cess sensitivity useful?  

\begin{figure}
% figure generated with cloud_table_AM4fig.ncl
  \includegraphics[width=0.86\columnwidth]{cfmipfigs/amip_p4k_future_cldfrac_calip.eps}
  \caption{Calipso scumstash}
  \label{fig:calipso_amip_comparison}
\end{figure}

\section{What are we learning?}

Experiments using Cess-like uniform SST warming as a proxy for climate change have successfully predicted that much of the differences 
among models is due to the cloud radiative affect.  However, we also know that the sensitivity derived from Cess-like experiments is not
a good predictor of the climate sensitivity for a parent, coupled model.  Comparison of Cess-sensitivities and coupled model sensitivities 
combined with patterned prescribed SST experiments using AGCM have demonstrated the significant role of a pattern-effect in determining 
the sensitivity of a model.  

We also know that a method which uses linear regression of a constant forcing experiment to estimate the climate sensitivity only 
provides an accurate measure of the climate sensitivity when the initial 20-50 years of simulation are excluded from the analysis.  
The climate sensitivities of models that have been run to equilibrium with millenial-scale simulations show a large discrepency in the
different time periods used to estimate this sensitivity.  

Clouds in a climate model are highly maleable while also being a dominant factor controlling what the climate sensitivity is for that model.  
Yet we do not have satisfactory clouds constraints to impose on the clouds of a climate model.  If improving the representation of low-level
clouds in the GFDL model was the first priority, it could be done.  However it is not the first priority and doing so degrades other elements of 
the climate.  

One of our goals was to highlight and learn from the influence of clouds feedbacks across a range of GFDL based models.  Our hope 
is that by looking at models from a single institution rather than a larger ensemble of CMIP models we will avoid inadvertantly 
masking important relationships by a multi-model ensemble mean.  

\end{document}
